{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0573111e",
   "metadata": {},
   "source": [
    "# Step 2: LLM Queries – Set 1 (A1–A7) and Set 2 (B1–B7)\n",
    "\n",
    "This notebook runs **both** prompt sets in one place:\n",
    "- **Set 1**: A1 (FSN), A2 (semantic tag), A3 (definition status), A4–A7 (parents, grandparents, children, siblings).\n",
    "- **Set 2**: B1 (official name), B2 (kind), B3 (category), B4–B7 (broader, grandparents, narrower, peers).\n",
    "\n",
    "**Prerequisites**: Run `step1_ground_truth.ipynb` in the `ground_truth/` folder first.\n",
    "\n",
    "Run **Step 3** (`step3_accuracy.ipynb`) after this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c93cd",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d3ea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_DIR: C:\\Users\\kanik\\OneDrive\\Documents\\llm_as_ontology_server\\llm-as-ontology-server\\output\\gemini\\run_001\n",
      "GT_ROOT: C:\\Users\\kanik\\OneDrive\\Documents\\llm_as_ontology_server\\llm-as-ontology-server\\output\\ground_truth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "\n",
    "# --- Detect LLM folder and paths ---\n",
    "_cwd = Path(\".\").resolve()\n",
    "if _cwd.name.startswith(\"testing_\"):\n",
    "    REPO_ROOT = _cwd.parent\n",
    "    LLM_NAME = _cwd.name.replace(\"testing_\", \"\")\n",
    "else:\n",
    "    REPO_ROOT = _cwd\n",
    "    LLM_NAME = \"gemini\"\n",
    "\n",
    "OUTPUT_ROOT = (REPO_ROOT / \"output\").resolve()\n",
    "PIPELINE_ROOT = OUTPUT_ROOT / LLM_NAME\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "PIPELINE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Shared ground truth (from step1_ground_truth.ipynb in ground_truth/ folder)\n",
    "GT_ROOT = OUTPUT_ROOT / \"ground_truth\"\n",
    "\n",
    "# Create NEW run directory for this LLM\n",
    "existing_runs = [d for d in PIPELINE_ROOT.iterdir() if d.is_dir() and d.name.startswith(\"run_\")]\n",
    "RUN_ID = len(existing_runs) + 1\n",
    "RUN_DIR = PIPELINE_ROOT / f\"run_{RUN_ID:03d}\"\n",
    "\n",
    "# Step directories\n",
    "SET1_DIR = RUN_DIR / \"step2_llm_set1\"\n",
    "SET2_DIR = RUN_DIR / \"step2_llm_set2\"\n",
    "\n",
    "# Output files\n",
    "SET1_OUT = SET1_DIR / \"set1_llm_output.csv\"\n",
    "SET2_OUT = SET2_DIR / \"set2_llm_output.csv\"\n",
    "\n",
    "# Create directories\n",
    "for d in [SET1_DIR, SET2_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n",
    "print(\"GT_ROOT:\", GT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1bbfb",
   "metadata": {},
   "source": [
    "## Load Validated Concepts from Shared Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee99368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total concepts to query: 400\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load validated concepts from shared Ground Truth (Step 1)\n",
    "# ============================================================\n",
    "\n",
    "VALIDATED_CONCEPTS_PATH = GT_ROOT / \"validated_concepts.csv\"\n",
    "if not VALIDATED_CONCEPTS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Validated concepts not found at {VALIDATED_CONCEPTS_PATH}\\n\"\n",
    "        f\"Run step1_ground_truth.ipynb in the ground_truth/ folder first!\"\n",
    "    )\n",
    "\n",
    "val_df = pd.read_csv(VALIDATED_CONCEPTS_PATH, dtype=str).fillna(\"\")\n",
    "CONCEPT_TERMS = val_df[\"concept_term\"].tolist()\n",
    "\n",
    "# Show replacement summary\n",
    "replaced = val_df[val_df[\"status\"] == \"replaced\"]\n",
    "if not replaced.empty:\n",
    "    print(\"Replaced concepts (original -> replacement):\")\n",
    "    for _, row in replaced.iterrows():\n",
    "        print(f\"  {row['original_term']} -> {row['concept_term']}\")\n",
    "\n",
    "print(f\"\\nTotal concepts to query: {len(CONCEPT_TERMS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda20bbf",
   "metadata": {},
   "source": [
    "## Set 1 – Prompt and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddf60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert SET1_DIR.exists(), \"SET1_DIR missing.\"\n",
    "LOG_PATH_SET1 = SET1_DIR / \"logs.txt\"\n",
    "MODEL_NAME = \"gemini-1.5-pro\"  # Gemini model\n",
    "PROMPT_TEMPLATE_SET1 = \"\"\"\n",
    "You are acting as a SNOMED CT ontology browser.\n",
    "\n",
    "Given the concept: \"{CONCEPT_TERM}\"\n",
    "\n",
    "Return ONLY the following fields.\n",
    "Use ONLY \"is-a\" taxonomic relationships.\n",
    "Do NOT explain anything.\n",
    "\n",
    "A1) FSN-style name (include semantic tag)\n",
    "A2) Semantic tag\n",
    "A3) Definition status (Primitive / Fully defined)\n",
    "A4) Immediate parent concept(s) (depth -1)\n",
    "A5) Grandparent concept(s) (depth -2, parents of parents)\n",
    "A6) Immediate child concept(s) (depth +1)\n",
    "A7) Near siblings (same parent)\n",
    "\n",
    "Rules:\n",
    "- Bullet lists for A4–A7\n",
    "- Exact labels A1–A7\n",
    "- No extra text\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c20f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _csv_safe(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    return str(x).replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "A_LABELS = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\"]\n",
    "\n",
    "def parse_A1_A7(raw: str) -> dict:\n",
    "    text = (raw or \"\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "    out = {k: \"\" for k in A_LABELS}\n",
    "    label_re = re.compile(r\"(?m)^\\s*(A[1-7])\\)\\s*(.*)$\")\n",
    "    matches = list(label_re.finditer(text))\n",
    "    if not matches:\n",
    "        return out\n",
    "    idx = {m.group(1): {\"start\": m.start(), \"after\": m.group(2).strip()} for m in matches}\n",
    "    def section(label):\n",
    "        if label not in idx:\n",
    "            return \"\"\n",
    "        start = idx[label][\"start\"]\n",
    "        ends = [idx[k][\"start\"] for k in idx if idx[k][\"start\"] > start]\n",
    "        end = min(ends) if ends else len(text)\n",
    "        return text[start:end]\n",
    "    for k in [\"A1\", \"A2\", \"A3\"]:\n",
    "        val = idx[k][\"after\"] if k in idx else \"\"\n",
    "        out[k] = val.strip() if val else \"\"\n",
    "    bullet_re = re.compile(r\"(?m)^\\s*[-*•]\\s+(.*)$\")\n",
    "    for k in [\"A4\", \"A5\", \"A6\", \"A7\"]:\n",
    "        block = section(k)\n",
    "        items = [m.group(1).strip() for m in bullet_re.finditer(block)]\n",
    "        items = [i.replace(\"|\", \" \") for i in items]\n",
    "        out[k] = \"|\".join(items) if items else \"\"\n",
    "    return out\n",
    "\n",
    "B_LABELS = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"]\n",
    "\n",
    "def parse_B1_B7(raw: str) -> dict:\n",
    "    text = (raw or \"\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "    out = {k: \"\" for k in B_LABELS}\n",
    "    label_re = re.compile(r\"(?m)^\\s*(B[1-7])\\)\\s*(.*)$\")\n",
    "    matches = list(label_re.finditer(text))\n",
    "    if not matches:\n",
    "        return out\n",
    "    idx = {m.group(1): {\"start\": m.start(), \"after\": m.group(2).strip()} for m in matches}\n",
    "    def section(label):\n",
    "        if label not in idx:\n",
    "            return \"\"\n",
    "        start = idx[label][\"start\"]\n",
    "        ends = [idx[k][\"start\"] for k in idx if idx[k][\"start\"] > start]\n",
    "        end = min(ends) if ends else len(text)\n",
    "        return text[start:end]\n",
    "    for k in [\"B1\", \"B2\", \"B3\"]:\n",
    "        val = idx[k][\"after\"] if k in idx else \"\"\n",
    "        out[k] = val.strip() if val else \"\"\n",
    "    bullet_re = re.compile(r\"(?m)^\\s*[-*•]\\s+(.*)$\")\n",
    "    for k in [\"B4\", \"B5\", \"B6\", \"B7\"]:\n",
    "        block = section(k)\n",
    "        items = [m.group(1).strip() for m in bullet_re.finditer(block)]\n",
    "        items = [i.replace(\"|\", \" \") for i in items]\n",
    "        out[k] = \"|\".join(items) if items else \"\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ad2f2",
   "metadata": {},
   "source": [
    "## Initialize LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075334b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\kanik\\AppData\\Local\\Temp\\ipykernel_39852\\3870192717.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      3\u001b[39m genai.configure(api_key=\u001b[33m\"\u001b[39m\u001b[33mAIzaSyDgc5huVgEYITi94V9rTuOstmAiWmgC63o\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY is not set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m model = genai.GenerativeModel(MODEL_NAME)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.generativeai' has no attribute 'api_key'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "if not genai.api_key:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY is not set.\")\n",
    "\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "print(\"Gemini client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd9f3a",
   "metadata": {},
   "source": [
    "## Set 1 – Resume and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa997a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SET1_OUT.exists():\n",
    "    existing_df_set1 = pd.read_csv(SET1_OUT, dtype=str).fillna(\"\")\n",
    "    done_terms_set1 = set(existing_df_set1[\"concept_term\"].tolist())\n",
    "else:\n",
    "    existing_df_set1 = pd.DataFrame()\n",
    "    done_terms_set1 = set()\n",
    "print(f\"Set 1 – concepts remaining: {len(CONCEPT_TERMS) - len(done_terms_set1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_set1 = []\n",
    "\n",
    "for concept_term in CONCEPT_TERMS:\n",
    "    if concept_term in done_terms_set1:\n",
    "        with LOG_PATH_SET1.open(\"a\") as f:\n",
    "            f.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tSKIP\\n\")\n",
    "        continue\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE_SET1.format(CONCEPT_TERM=concept_term)\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        raw_output = response.text or \"\"\n",
    "        parsed = parse_A1_A7(raw_output)\n",
    "\n",
    "        rows_set1.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt_set\": \"set1\",\n",
    "            \"concept_term\": concept_term,\n",
    "            \"A1_fsn\": _csv_safe(parsed[\"A1\"]),\n",
    "            \"A2_semantic_tag\": _csv_safe(parsed[\"A2\"]),\n",
    "            \"A3_definition_status\": _csv_safe(parsed[\"A3\"]),\n",
    "            \"A4_parents\": _csv_safe(parsed[\"A4\"]),\n",
    "            \"A5_grandparents\": _csv_safe(parsed[\"A5\"]),\n",
    "            \"A6_children\": _csv_safe(parsed[\"A6\"]),\n",
    "            \"A7_siblings\": _csv_safe(parsed[\"A7\"]),\n",
    "        })\n",
    "\n",
    "        with LOG_PATH_SET1.open(\"a\") as f:\n",
    "            f.write(\n",
    "                \"\\n\" + \"=\"*80 + \"\\n\" + datetime.now().isoformat() + \"\\n\"\n",
    "                + \"CONCEPT: \" + concept_term + \"\\n\\n\" + raw_output.strip() + \"\\n\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        with LOG_PATH_SET1.open(\"a\") as f:\n",
    "            f.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tERROR\\t{e}\\n\")\n",
    "\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f\"Set 1 processed {len(rows_set1)} new concepts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rows_set1:\n",
    "    out_df = pd.DataFrame(rows_set1)\n",
    "    combined = pd.concat([existing_df_set1, out_df], ignore_index=True) if not existing_df_set1.empty else out_df\n",
    "    combined = combined[[\"timestamp\", \"model\", \"prompt_set\", \"concept_term\", \"A1_fsn\", \"A2_semantic_tag\", \"A3_definition_status\", \"A4_parents\", \"A5_grandparents\", \"A6_children\", \"A7_siblings\"]]\n",
    "    combined.to_csv(SET1_OUT, index=False)\n",
    "print(\"Set 1 complete.\", SET1_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340207d",
   "metadata": {},
   "source": [
    "## Set 2 – Prompt, Resume and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert SET2_DIR.exists(), \"SET2_DIR missing.\"\n",
    "LOG_PATH_SET2 = SET2_DIR / \"logs.txt\"\n",
    "PROMPT_TEMPLATE_SET2 = \"\"\"\n",
    "For the term: \"{CONCEPT_TERM}\"\n",
    "\n",
    "Answer ONLY with the items below.\n",
    "Do NOT explain.\n",
    "\n",
    "B1) Most precise official-style name\n",
    "B2) What kind of thing it is (semantic type)\n",
    "B3) Category type (Primitive / Fully defined)\n",
    "B4) More general terms (immediate broader concepts)\n",
    "B5) Grandparent terms (broader concepts two levels up)\n",
    "B6) More specific terms (immediate narrower concepts)\n",
    "B7) Terms at the same generality level (peers / siblings)\n",
    "\n",
    "Rules:\n",
    "- Bullet lists where applicable\n",
    "- Exact labels B1–B7\n",
    "- No extra text\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SET2_OUT.exists():\n",
    "    existing_df_set2 = pd.read_csv(SET2_OUT, dtype=str).fillna(\"\")\n",
    "    done_terms_set2 = set(existing_df_set2[\"concept_term\"].tolist())\n",
    "else:\n",
    "    existing_df_set2 = pd.DataFrame()\n",
    "    done_terms_set2 = set()\n",
    "print(f\"Set 2 – concepts remaining: {len(CONCEPT_TERMS) - len(done_terms_set2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_set2 = []\n",
    "\n",
    "for concept_term in CONCEPT_TERMS:\n",
    "    if concept_term in done_terms_set2:\n",
    "        with LOG_PATH_SET2.open(\"a\") as f:\n",
    "            f.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tSKIP\\n\")\n",
    "        continue\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE_SET2.format(CONCEPT_TERM=concept_term)\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        raw_output = response.text or \"\"\n",
    "        parsed = parse_B1_B7(raw_output)\n",
    "\n",
    "        rows_set2.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt_set\": \"set2\",\n",
    "            \"concept_term\": concept_term,\n",
    "            \"B1_official_name\": _csv_safe(parsed[\"B1\"]),\n",
    "            \"B2_kind\": _csv_safe(parsed[\"B2\"]),\n",
    "            \"B3_category_type\": _csv_safe(parsed[\"B3\"]),\n",
    "            \"B4_immediate_broader\": _csv_safe(parsed[\"B4\"]),\n",
    "            \"B5_grandparents\": _csv_safe(parsed[\"B5\"]),\n",
    "            \"B6_immediate_narrower\": _csv_safe(parsed[\"B6\"]),\n",
    "            \"B7_peer_terms\": _csv_safe(parsed[\"B7\"]),\n",
    "        })\n",
    "\n",
    "        with LOG_PATH_SET2.open(\"a\") as f:\n",
    "            f.write(\n",
    "                \"\\n\" + \"=\"*80 + \"\\n\" + datetime.now().isoformat() + \"\\n\"\n",
    "                + \"CONCEPT: \" + concept_term + \"\\n\\n\" + raw_output.strip() + \"\\n\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        with LOG_PATH_SET2.open(\"a\") as f:\n",
    "            f.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tERROR\\t{e}\\n\")\n",
    "\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f\"Set 2 processed {len(rows_set2)} new concepts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rows_set2:\n",
    "    out_df = pd.DataFrame(rows_set2)\n",
    "    combined = pd.concat([existing_df_set2, out_df], ignore_index=True) if not existing_df_set2.empty else out_df\n",
    "    combined = combined[[\"timestamp\", \"model\", \"prompt_set\", \"concept_term\", \"B1_official_name\", \"B2_kind\", \"B3_category_type\", \"B4_immediate_broader\", \"B5_grandparents\", \"B6_immediate_narrower\", \"B7_peer_terms\"]]\n",
    "    combined.to_csv(SET2_OUT, index=False)\n",
    "print(\"Set 2 complete.\", SET2_OUT)\n",
    "print(\"Run step3_accuracy.ipynb next.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
