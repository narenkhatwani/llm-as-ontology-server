{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Config\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# --- User-editable base path ---\n",
    "SNOMED_BASE = Path(\"/PATH/TO/YOUR/SNOMED_BASE\")  # e.g., \"/Users/you/.../SnomedCT_files\"\n",
    "\n",
    "# --- Run directory ---\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "PIPELINE_ROOT = SNOMED_BASE / \"_outputs_llm_eval\"\n",
    "RUN_DIR = PIPELINE_ROOT / f\"run_{RUN_ID}\"\n",
    "\n",
    "STEP1_DIR = RUN_DIR / \"step1_llm_set1\"\n",
    "STEP2_DIR = RUN_DIR / \"step2_llm_set2\"\n",
    "STEP3_DIR = RUN_DIR / \"step3_snomed_ground_truth\"\n",
    "STEP4_DIR = RUN_DIR / \"step4_comparison\"\n",
    "\n",
    "# Output files\n",
    "STEP1_OUT = STEP1_DIR / \"set1_llm_output.csv\"\n",
    "STEP2_OUT = STEP2_DIR / \"set2_llm_output.csv\"\n",
    "STEP3_OUT = STEP3_DIR / \"snomed_ground_truth.csv\"\n",
    "\n",
    "STEP4_OUT_MAIN = STEP4_DIR / \"comparison_results.csv\"\n",
    "STEP4_OUT_LONG = STEP4_DIR / \"comparison_results_long.csv\"\n",
    "STEP4_OUT_WIDE = STEP4_DIR / \"comparison_results_wide.csv\"\n",
    "STEP4_OUT_TSV = STEP4_DIR / \"comparison_results.tsv\"\n",
    "\n",
    "# Create directories\n",
    "for d in [STEP1_DIR, STEP2_DIR, STEP3_DIR, STEP4_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Concept terms (replace with your level 3/4 concepts)\n",
    "CONCEPT_TERMS = [\n",
    "    \"Hypertensive disorder\",\n",
    "    \"Type 2 diabetes mellitus\",\n",
    "    \"Asthma\",\n",
    "]\n",
    "\n",
    "print(\"Run directory:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: LLM querying - Set 1\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "assert STEP1_DIR.exists(), \"Run the Config cell first (STEP1_DIR missing).\"\n",
    "\n",
    "LOG_PATH = STEP1_DIR / \"logs.txt\"\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"  # Change if needed\n",
    "PROMPT_SET = \"set1\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a SNOMED taxonomy assistant. For the concept term: \"{concept_term}\", return STRICT JSON only:\n",
    "{{\"sons\": [...], \"cousins\": [...]}}\n",
    "\n",
    "Definitions:\n",
    "- \"sons\": direct is-a children of the concept.\n",
    "- \"cousins\": children of siblings / nearby lateral concepts.\n",
    "\n",
    "No explanations. JSON only.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def normalize_term(term: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", term.strip().lower())\n",
    "\n",
    "\n",
    "def safe_json_extract(text: str) -> dict:\n",
    "    # Extract the first JSON object found in the response\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response.\")\n",
    "    json_str = match.group(0)\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "def list_to_pipe(items):\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    return \"|\".join([str(i).replace(\"|\", \" \") for i in items])\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "if not client.api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set in environment.\")\n",
    "\n",
    "# Load existing output for resumability\n",
    "if STEP1_OUT.exists():\n",
    "    existing_df = pd.read_csv(STEP1_OUT, dtype=str).fillna(\"\")\n",
    "    done_terms = set(existing_df[\"concept_term\"].astype(str).tolist())\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    done_terms = set()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for concept_term in CONCEPT_TERMS:\n",
    "    if concept_term in done_terms:\n",
    "        with LOG_PATH.open(\"a\") as logf:\n",
    "            logf.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tSKIP (already processed)\\n\")\n",
    "        continue\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(concept_term=concept_term)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content or \"\"\n",
    "        parsed = safe_json_extract(raw_output)\n",
    "        sons = parsed.get(\"sons\", [])\n",
    "        cousins = parsed.get(\"cousins\", [])\n",
    "\n",
    "        row = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt_set\": PROMPT_SET,\n",
    "            \"concept_term\": concept_term,\n",
    "            \"raw_output\": raw_output.replace(\"\\n\", \" \").strip(),\n",
    "            \"extracted_sons\": list_to_pipe(sons),\n",
    "            \"extracted_cousins\": list_to_pipe(cousins),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        with LOG_PATH.open(\"a\") as logf:\n",
    "            logf.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tOK\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with LOG_PATH.open(\"a\") as logf:\n",
    "            logf.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tERROR\\t{e}\\n\")\n",
    "\n",
    "    time.sleep(0.2)\n",
    "\n",
    "# Append results\n",
    "if rows:\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    if STEP1_OUT.exists():\n",
    "        combined = pd.concat([existing_df, out_df], ignore_index=True)\n",
    "    else:\n",
    "        combined = out_df\n",
    "    combined.to_csv(STEP1_OUT, index=False)\n",
    "\n",
    "print(\"Step 1 complete. Output:\", STEP1_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: LLM querying - Set 2\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "assert STEP2_DIR.exists(), \"Run the Config cell first (STEP2_DIR missing).\"\n",
    "\n",
    "LOG_PATH = STEP2_DIR / \"logs.txt\"\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"  # Change if needed\n",
    "PROMPT_SET = \"set2\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a SNOMED taxonomy assistant. For the concept term: \"{concept_term}\", return STRICT JSON only:\n",
    "{{\"sons\": [...], \"cousins\": [...]}}\n",
    "\n",
    "Constraints:\n",
    "- Up to 30 sons and 30 cousins.\n",
    "- Prefer SNOMED-like clinically common terms.\n",
    "\n",
    "Definitions:\n",
    "- \"sons\": direct is-a children of the concept.\n",
    "- \"cousins\": children of siblings / nearby lateral concepts.\n",
    "\n",
    "No explanations. JSON only.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def normalize_term(term: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", term.strip().lower())\n",
    "\n",
    "\n",
    "def safe_json_extract(text: str) -> dict:\n",
    "    # Extract the first JSON object found in the response\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response.\")\n",
    "    json_str = match.group(0)\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "def list_to_pipe(items):\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    return \"|\".join([str(i).replace(\"|\", \" \") for i in items])\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "if not client.api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set in environment.\")\n",
    "\n",
    "# Load existing output for resumability\n",
    "if STEP2_OUT.exists():\n",
    "    existing_df = pd.read_csv(STEP2_OUT, dtype=str).fillna(\"\")\n",
    "    done_terms = set(existing_df[\"concept_term\"].astype(str).tolist())\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    done_terms = set()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for concept_term in CONCEPT_TERMS:\n",
    "    if concept_term in done_terms:\n",
    "        with LOG_PATH.open(\"a\") as logf:\n",
    "            logf.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tSKIP (already processed)\\n\")\n",
    "        continue\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(concept_term=concept_term)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content or \"\"\n",
    "        parsed = safe_json_extract(raw_output)\n",
    "        sons = parsed.get(\"sons\", [])\n",
    "        cousins = parsed.get(\"cousins\", [])\n",
    "\n",
    "        row = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt_set\": PROMPT_SET,\n",
    "            \"concept_term\": concept_term,\n",
    "            \"raw_output\": raw_output.replace(\"\\n\", \" \").strip(),\n",
    "            \"extracted_sons\": list_to_pipe(sons),\n",
    "            \"extracted_cousins\": list_to_pipe(cousins),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        with LOG_PATH.open(\"a\") as logf:\n",
    "            logf.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tOK\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with LOG_PATH.open(\"a\") as logf:\n",
    "            logf.write(f\"{datetime.now().isoformat()}\\t{concept_term}\\tERROR\\t{e}\\n\")\n",
    "\n",
    "    time.sleep(0.2)\n",
    "\n",
    "# Append results\n",
    "if rows:\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    if STEP2_OUT.exists():\n",
    "        combined = pd.concat([existing_df, out_df], ignore_index=True)\n",
    "    else:\n",
    "        combined = out_df\n",
    "    combined.to_csv(STEP2_OUT, index=False)\n",
    "\n",
    "print(\"Step 2 complete. Output:\", STEP2_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build SNOMED ground truth\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Input RF2 paths (US Edition snapshot)\n",
    "DESC_PATH = SNOMED_BASE / \"Snapshot\" / \"Terminology\" / \"sct2_Description_Snapshot-en_US1000124_20250901.txt\"\n",
    "REL_PATH = SNOMED_BASE / \"Snapshot\" / \"Terminology\" / \"sct2_Relationship_Snapshot_US1000124_20250901.txt\"  # inferred\n",
    "\n",
    "assert DESC_PATH.exists(), f\"Missing description file: {DESC_PATH}\"\n",
    "assert REL_PATH.exists(), f\"Missing relationship file: {REL_PATH}\"\n",
    "assert STEP3_DIR.exists(), \"Run the Config cell first (STEP3_DIR missing).\"\n",
    "\n",
    "ROOT_CONCEPT_ID = \"138875005\"  # SNOMED CT root concept\n",
    "\n",
    "\n",
    "def normalize_term(term: str) -> str:\n",
    "    term = term.strip().lower()\n",
    "    term = re.sub(r\"\\s+\", \" \", term)\n",
    "    return term\n",
    "\n",
    "\n",
    "# --- Load descriptions ---\n",
    "# RF2 description fields: id, effectiveTime, active, moduleId, conceptId, languageCode, typeId, term, caseSignificanceId\n",
    "# We'll use active==1\n",
    "\n",
    "fsn_by_concept = {}\n",
    "terms_by_concept = defaultdict(list)\n",
    "conceptids_by_term = defaultdict(set)\n",
    "\n",
    "with DESC_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    header = f.readline().rstrip(\"\\n\").split(\"\\t\")\n",
    "    idx = {name: i for i, name in enumerate(header)}\n",
    "\n",
    "    for line in f:\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        if parts[idx[\"active\"]] != \"1\":\n",
    "            continue\n",
    "        concept_id = parts[idx[\"conceptId\"]]\n",
    "        term = parts[idx[\"term\"]]\n",
    "        type_id = parts[idx[\"typeId\"]]  # FSN if 900000000000003001\n",
    "\n",
    "        if type_id == \"900000000000003001\":\n",
    "            fsn_by_concept[concept_id] = term\n",
    "\n",
    "        terms_by_concept[concept_id].append(term)\n",
    "        conceptids_by_term[normalize_term(term)].add(concept_id)\n",
    "\n",
    "\n",
    "# --- Load inferred is-a relationships ---\n",
    "# RF2 relationship fields: id, effectiveTime, active, moduleId, sourceId, destinationId, relationshipGroup,\n",
    "# typeId, characteristicTypeId, modifierId\n",
    "\n",
    "IS_A_TYPE_ID = \"116680003\"\n",
    "\n",
    "parents_by_child = defaultdict(set)\n",
    "children_by_parent = defaultdict(set)\n",
    "\n",
    "with REL_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    header = f.readline().rstrip(\"\\n\").split(\"\\t\")\n",
    "    idx = {name: i for i, name in enumerate(header)}\n",
    "\n",
    "    for line in f:\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        if parts[idx[\"active\"]] != \"1\":\n",
    "            continue\n",
    "        if parts[idx[\"typeId\"]] != IS_A_TYPE_ID:\n",
    "            continue\n",
    "\n",
    "        child_id = parts[idx[\"sourceId\"]]\n",
    "        parent_id = parts[idx[\"destinationId\"]]\n",
    "\n",
    "        parents_by_child[child_id].add(parent_id)\n",
    "        children_by_parent[parent_id].add(child_id)\n",
    "\n",
    "\n",
    "# --- Compute depth from root ---\n",
    "# BFS from ROOT_CONCEPT_ID\n",
    "\n",
    "depth_by_concept = {}\n",
    "queue = deque([(ROOT_CONCEPT_ID, 0)])\n",
    "visited = set([ROOT_CONCEPT_ID])\n",
    "\n",
    "while queue:\n",
    "    cid, d = queue.popleft()\n",
    "    depth_by_concept[cid] = d\n",
    "    for child in children_by_parent.get(cid, []):\n",
    "        if child not in visited:\n",
    "            visited.add(child)\n",
    "            queue.append((child, d + 1))\n",
    "\n",
    "\n",
    "# --- Resolve concept term to best conceptId ---\n",
    "def resolve_concept_id(concept_term: str):\n",
    "    candidates = list(conceptids_by_term.get(normalize_term(concept_term), []))\n",
    "    if not candidates:\n",
    "        return None, \"No match in term index\"\n",
    "\n",
    "    # Prefer concepts with depth computed and FSN available, then smaller depth\n",
    "    def score(cid):\n",
    "        has_depth = cid in depth_by_concept\n",
    "        has_fsn = cid in fsn_by_concept\n",
    "        depth = depth_by_concept.get(cid, 10**9)\n",
    "        return (0 if has_depth else 1, 0 if has_fsn else 1, depth)\n",
    "\n",
    "    candidates.sort(key=score)\n",
    "    return candidates[0], None\n",
    "\n",
    "\n",
    "def ids_to_terms(ids):\n",
    "    terms = []\n",
    "    for cid in ids:\n",
    "        terms.append(fsn_by_concept.get(cid, terms_by_concept.get(cid, [cid])[0] if terms_by_concept.get(cid) else cid))\n",
    "    return terms\n",
    "\n",
    "\n",
    "rows = []\n",
    "for concept_term in CONCEPT_TERMS:\n",
    "    concept_id, note = resolve_concept_id(concept_term)\n",
    "    if not concept_id:\n",
    "        rows.append({\n",
    "            \"concept_term\": concept_term,\n",
    "            \"snomed_id\": \"\",\n",
    "            \"fsn\": \"\",\n",
    "            \"depth\": \"\",\n",
    "            \"sons_ids\": \"\",\n",
    "            \"sons_terms\": \"\",\n",
    "            \"sons_count\": 0,\n",
    "            \"cousins_ids\": \"\",\n",
    "            \"cousins_terms\": \"\",\n",
    "            \"cousins_count\": 0,\n",
    "            \"note\": note,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    sons_ids = sorted(children_by_parent.get(concept_id, []))\n",
    "\n",
    "    # Siblings: other children of each parent\n",
    "    siblings = set()\n",
    "    for parent in parents_by_child.get(concept_id, []):\n",
    "        siblings.update(children_by_parent.get(parent, []))\n",
    "    siblings.discard(concept_id)\n",
    "\n",
    "    cousins_ids = set()\n",
    "    for sibling_id in siblings:\n",
    "        cousins_ids.update(children_by_parent.get(sibling_id, []))\n",
    "\n",
    "    sons_terms = ids_to_terms(sons_ids)\n",
    "    cousins_terms = ids_to_terms(sorted(cousins_ids))\n",
    "\n",
    "    rows.append({\n",
    "        \"concept_term\": concept_term,\n",
    "        \"snomed_id\": concept_id,\n",
    "        \"fsn\": fsn_by_concept.get(concept_id, \"\"),\n",
    "        \"depth\": depth_by_concept.get(concept_id, \"\"),\n",
    "        \"sons_ids\": \"|\".join([str(x) for x in sons_ids]),\n",
    "        \"sons_terms\": \"|\".join([t.replace(\"|\", \" \") for t in sons_terms]),\n",
    "        \"sons_count\": len(sons_ids),\n",
    "        \"cousins_ids\": \"|\".join([str(x) for x in sorted(cousins_ids)]),\n",
    "        \"cousins_terms\": \"|\".join([t.replace(\"|\", \" \") for t in cousins_terms]),\n",
    "        \"cousins_count\": len(cousins_ids),\n",
    "        \"note\": note or \"\",\n",
    "    })\n",
    "\n",
    "out_df = pd.DataFrame(rows)\n",
    "out_df.to_csv(STEP3_OUT, index=False)\n",
    "\n",
    "print(\"Step 3 complete. Output:\", STEP3_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare LLM outputs vs SNOMED\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "assert STEP1_OUT.exists(), f\"Missing Step 1 output: {STEP1_OUT}\"\n",
    "assert STEP2_OUT.exists(), f\"Missing Step 2 output: {STEP2_OUT}\"\n",
    "assert STEP3_OUT.exists(), f\"Missing Step 3 output: {STEP3_OUT}\"\n",
    "\n",
    "\n",
    "def normalize_term(term: str) -> str:\n",
    "    term = term.strip().lower()\n",
    "    term = re.sub(r\"\\s+\", \" \", term)\n",
    "    return term\n",
    "\n",
    "\n",
    "def split_pipe(s: str):\n",
    "    if not isinstance(s, str) or s.strip() == \"\":\n",
    "        return []\n",
    "    return [x.strip() for x in s.split(\"|\") if x.strip()]\n",
    "\n",
    "\n",
    "def list_to_pipe(items):\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    return \"|\".join([str(i).replace(\"|\", \" \") for i in items])\n",
    "\n",
    "\n",
    "# Load inputs\n",
    "set1 = pd.read_csv(STEP1_OUT, dtype=str).fillna(\"\")\n",
    "set2 = pd.read_csv(STEP2_OUT, dtype=str).fillna(\"\")\n",
    "truth = pd.read_csv(STEP3_OUT, dtype=str).fillna(\"\")\n",
    "\n",
    "truth_map = {row[\"concept_term\"]: row for _, row in truth.iterrows()}\n",
    "\n",
    "\n",
    "def compare_row(prompt_set: str, row: pd.Series, truth_row: pd.Series):\n",
    "    llm_sons = split_pipe(row.get(\"extracted_sons\", \"\"))\n",
    "    llm_cousins = split_pipe(row.get(\"extracted_cousins\", \"\"))\n",
    "\n",
    "    snomed_sons = split_pipe(truth_row.get(\"sons_terms\", \"\"))\n",
    "    snomed_cousins = split_pipe(truth_row.get(\"cousins_terms\", \"\"))\n",
    "\n",
    "    # Normalize for matching\n",
    "    llm_sons_norm = {normalize_term(x) for x in llm_sons}\n",
    "    llm_cousins_norm = {normalize_term(x) for x in llm_cousins}\n",
    "    snomed_sons_norm = {normalize_term(x) for x in snomed_sons}\n",
    "    snomed_cousins_norm = {normalize_term(x) for x in snomed_cousins}\n",
    "\n",
    "    sons_intersection = sorted(snomed_sons_norm & llm_sons_norm)\n",
    "    sons_missed = sorted(snomed_sons_norm - llm_sons_norm)\n",
    "    sons_extra = sorted(llm_sons_norm - snomed_sons_norm)\n",
    "\n",
    "    cousins_intersection = sorted(snomed_cousins_norm & llm_cousins_norm)\n",
    "    cousins_missed = sorted(snomed_cousins_norm - llm_cousins_norm)\n",
    "    cousins_extra = sorted(llm_cousins_norm - snomed_cousins_norm)\n",
    "\n",
    "    def safe_div(a, b):\n",
    "        return (a / b) if b else 0.0\n",
    "\n",
    "    sons_correct = len(sons_intersection)\n",
    "    sons_count = len(snomed_sons_norm)\n",
    "    sons_llm_count = len(llm_sons_norm)\n",
    "\n",
    "    cousins_correct = len(cousins_intersection)\n",
    "    cousins_count = len(snomed_cousins_norm)\n",
    "    cousins_llm_count = len(llm_cousins_norm)\n",
    "\n",
    "    return {\n",
    "        \"prompt_set\": prompt_set,\n",
    "        \"concept_term\": row[\"concept_term\"],\n",
    "        \"llm_sons_raw\": list_to_pipe(llm_sons),\n",
    "        \"llm_cousins_raw\": list_to_pipe(llm_cousins),\n",
    "        \"snomed_sons_terms\": list_to_pipe(snomed_sons),\n",
    "        \"snomed_cousins_terms\": list_to_pipe(snomed_cousins),\n",
    "        \"sons_intersection\": list_to_pipe(sons_intersection),\n",
    "        \"sons_missed\": list_to_pipe(sons_missed),\n",
    "        \"sons_extra\": list_to_pipe(sons_extra),\n",
    "        \"cousins_intersection\": list_to_pipe(cousins_intersection),\n",
    "        \"cousins_missed\": list_to_pipe(cousins_missed),\n",
    "        \"cousins_extra\": list_to_pipe(cousins_extra),\n",
    "        \"snomed_sons_count\": sons_count,\n",
    "        \"llm_sons_count\": sons_llm_count,\n",
    "        \"sons_correct\": sons_correct,\n",
    "        \"sons_missed_count\": len(sons_missed),\n",
    "        \"sons_extra_count\": len(sons_extra),\n",
    "        \"sons_recall\": safe_div(sons_correct, sons_count),\n",
    "        \"sons_precision\": safe_div(sons_correct, sons_llm_count),\n",
    "        \"snomed_cousins_count\": cousins_count,\n",
    "        \"llm_cousins_count\": cousins_llm_count,\n",
    "        \"cousins_correct\": cousins_correct,\n",
    "        \"cousins_missed_count\": len(cousins_missed),\n",
    "        \"cousins_extra_count\": len(cousins_extra),\n",
    "        \"cousins_recall\": safe_div(cousins_correct, cousins_count),\n",
    "        \"cousins_precision\": safe_div(cousins_correct, cousins_llm_count),\n",
    "        \"missing_in_llm_csv\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process both prompt sets\n",
    "for df, prompt_set in [(set1, \"set1\"), (set2, \"set2\")]:\n",
    "    for _, row in df.iterrows():\n",
    "        concept_term = row[\"concept_term\"]\n",
    "        truth_row = truth_map.get(concept_term)\n",
    "        if truth_row is None:\n",
    "            results.append({\n",
    "                \"prompt_set\": prompt_set,\n",
    "                \"concept_term\": concept_term,\n",
    "                \"missing_in_llm_csv\": True,\n",
    "            })\n",
    "            continue\n",
    "        results.append(compare_row(prompt_set, row, truth_row))\n",
    "\n",
    "# Add missing rows for any concept_terms not in LLM outputs\n",
    "for concept_term in CONCEPT_TERMS:\n",
    "    for prompt_set in [\"set1\", \"set2\"]:\n",
    "        if not any(r.get(\"prompt_set\") == prompt_set and r.get(\"concept_term\") == concept_term for r in results):\n",
    "            results.append({\n",
    "                \"prompt_set\": prompt_set,\n",
    "                \"concept_term\": concept_term,\n",
    "                \"missing_in_llm_csv\": True,\n",
    "            })\n",
    "\n",
    "comparison_df = pd.DataFrame(results).fillna(\"\")\n",
    "comparison_df.to_csv(STEP4_OUT_MAIN, index=False)\n",
    "comparison_df.to_csv(STEP4_OUT_TSV, index=False, sep=\"\\t\")\n",
    "\n",
    "# --- Long (tidy) format ---\n",
    "long_rows = []\n",
    "for _, row in comparison_df.iterrows():\n",
    "    if row.get(\"missing_in_llm_csv\") in [True, \"True\", \"true\", \"1\"]:\n",
    "        continue\n",
    "\n",
    "    for relation in [\"sons\", \"cousins\"]:\n",
    "        for status in [\"intersection\", \"missed\", \"extra\"]:\n",
    "            col = f\"{relation}_{status}\"\n",
    "            items = split_pipe(row.get(col, \"\"))\n",
    "            for item in items:\n",
    "                long_rows.append({\n",
    "                    \"prompt_set\": row[\"prompt_set\"],\n",
    "                    \"concept_term\": row[\"concept_term\"],\n",
    "                    \"relation\": relation,\n",
    "                    \"status\": \"correct\" if status == \"intersection\" else status,\n",
    "                    \"item_term\": item,\n",
    "                    \"snomed_id\": \"\",\n",
    "                    \"depth\": \"\",\n",
    "                })\n",
    "\n",
    "long_df = pd.DataFrame(long_rows)\n",
    "long_df.to_csv(STEP4_OUT_LONG, index=False)\n",
    "\n",
    "# --- Wide (Excel-friendly) format ---\n",
    "wide_rows = []\n",
    "for _, row in comparison_df.iterrows():\n",
    "    wide_row = row.to_dict()\n",
    "    for col in [\n",
    "        \"sons_intersection\", \"sons_missed\", \"sons_extra\",\n",
    "        \"cousins_intersection\", \"cousins_missed\", \"cousins_extra\",\n",
    "    ]:\n",
    "        items = split_pipe(row.get(col, \"\"))\n",
    "        for i, item in enumerate(items, start=1):\n",
    "            wide_row[f\"{col}_{i}\"] = item\n",
    "    wide_rows.append(wide_row)\n",
    "\n",
    "wide_df = pd.DataFrame(wide_rows)\n",
    "wide_df.to_csv(STEP4_OUT_WIDE, index=False)\n",
    "\n",
    "print(\"Step 4 complete. Outputs:\")\n",
    "print(\"-\", STEP4_OUT_MAIN)\n",
    "print(\"-\", STEP4_OUT_LONG)\n",
    "print(\"-\", STEP4_OUT_WIDE)\n",
    "print(\"-\", STEP4_OUT_TSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
