#!/usr/bin/env python3
"""
Build step1_ground_truth.ipynb in the ground_truth/ folder (repo root).
This notebook:
  1. Validates CONCEPT_TERMS against BioPortal SNOMED CT API
  2. Replaces NOT FOUND concepts with random SNOMED replacements
  3. Extracts ground truth (parents, grandparents, children, siblings)
  4. Saves validated_concepts.csv + ground_truth.csv

Runs ONCE and is shared by all LLM testing folders.
"""
import json
from pathlib import Path

REPO = Path(__file__).resolve().parent.parent

# Read CONCEPT_TERMS source from concept_list_helper.ipynb
_src_nb_path = REPO / "testing_gpt" / "concept_list_helper.ipynb"
with open(_src_nb_path) as f:
    _src_nb = json.load(f)
CONCEPT_SOURCE = None
for c in _src_nb["cells"]:
    if c["cell_type"] == "code" and "CONCEPT_TERMS = [" in "".join(c.get("source", [])):
        CONCEPT_SOURCE = c["source"]
        break
if not CONCEPT_SOURCE:
    raise SystemExit("Could not find CONCEPT_TERMS cell in concept_list_helper.ipynb")


def md(s):
    return {"cell_type": "markdown", "metadata": {}, "source": s.split("\n") if isinstance(s, str) else s}

def code(s):
    lines = s if isinstance(s, list) else s.split("\n")
    # Ensure each line ends with \n
    result = []
    for line in lines:
        if not line.endswith("\n"):
            result.append(line + "\n")
        else:
            result.append(line)
    # Remove trailing newline from last line
    if result and result[-1] == "\n":
        result = result[:-1]
    return {"cell_type": "code", "metadata": {}, "outputs": [], "source": result}


def build_cells():
    cells = []

    # ---- Title ----
    cells.append(md(
        "# Step 1: SNOMED Ground Truth Extraction (via BioPortal API)\n"
        "\n"
        "This notebook:\n"
        "1. Validates each concept term against the BioPortal SNOMED CT API\n"
        "2. Replaces NOT FOUND concepts with random valid SNOMED concepts\n"
        "3. Extracts ground truth relationships (parents, grandparents, children, siblings)\n"
        "4. Saves `validated_concepts.csv` and `ground_truth.csv`\n"
        "\n"
        "**Run this notebook ONCE** from the `ground_truth/` folder.\n"
        "The output is shared by all LLM testing folders (testing_gpt, testing_claude, etc.).\n"
        "\n"
        "After this, run `step2_llm_queries.ipynb` in each testing folder."
    ))

    # ---- Configuration ----
    cells.append(md("## Configuration"))
    cells.append(code(
        "import os\n"
        "import re\n"
        "import random\n"
        "import requests\n"
        "import time\n"
        "from datetime import datetime\n"
        "from pathlib import Path\n"
        "from urllib.parse import quote\n"
        "\n"
        "import pandas as pd\n"
        "\n"
        "# ============================================================\n"
        "# Path Configuration\n"
        "# ============================================================\n"
        "\n"
        "_cwd = Path(\".\").resolve()\n"
        "# This notebook lives in ground_truth/ at repo root\n"
        "if _cwd.name == \"ground_truth\":\n"
        "    REPO_ROOT = _cwd.parent\n"
        "else:\n"
        "    REPO_ROOT = _cwd  # running from repo root\n"
        "\n"
        "OUTPUT_ROOT = (REPO_ROOT / \"output\").resolve()\n"
        "GT_ROOT = OUTPUT_ROOT / \"ground_truth\"\n"
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n"
        "GT_ROOT.mkdir(parents=True, exist_ok=True)\n"
        "\n"
        "# Output files (shared across all LLMs)\n"
        "GT_OUT = GT_ROOT / \"ground_truth.csv\"\n"
        "VALIDATED_CONCEPTS_OUT = GT_ROOT / \"validated_concepts.csv\"\n"
        "LOG_DIR = GT_ROOT\n"
        "\n"
        "# Set to True to run on 5 random concepts only (for testing)\n"
        "TEST_MODE = True\n"
        "\n"
        "print(\"Output directory:\", GT_ROOT)\n"
    ))

    # ---- Concept Terms ----
    cells.append(md("## Concept Terms"))
    cells.append(code(CONCEPT_SOURCE))

    # ---- TEST_MODE filter ----
    cells.append(code(
        "if TEST_MODE:\n"
        "    import random as _rng\n"
        "    _rng.seed(42)\n"
        "    CONCEPT_TERMS = _rng.sample(CONCEPT_TERMS, min(5, len(CONCEPT_TERMS)))\n"
        "    print(f\"TEST MODE: using {len(CONCEPT_TERMS)} random concepts\")\n"
        "print(f\"Concepts to process: {len(CONCEPT_TERMS)}\")\n"
    ))

    # ---- Backup Concepts (for NOT FOUND replacements) ----
    cells.append(md(
        "## Backup Concepts\n"
        "\n"
        "Known-good SNOMED CT concepts used as replacements when a primary concept is NOT FOUND."
    ))
    cells.append(code(
        "BACKUP_CONCEPTS = [\n"
        "    \"Diabetes mellitus\",\n"
        "    \"Asthma\",\n"
        "    \"Pneumonia\",\n"
        "    \"Fracture of femur\",\n"
        "    \"Appendectomy\",\n"
        "    \"Aspirin allergy\",\n"
        "    \"Migraine\",\n"
        "    \"Osteoarthritis\",\n"
        "    \"Chronic kidney disease\",\n"
        "    \"Hypothyroidism\",\n"
        "    \"Rheumatoid arthritis\",\n"
        "    \"Epilepsy\",\n"
        "    \"Pulmonary embolism\",\n"
        "    \"Cellulitis\",\n"
        "    \"Otitis media\",\n"
        "    \"Cirrhosis of liver\",\n"
        "    \"Gout\",\n"
        "    \"Psoriasis\",\n"
        "    \"Endoscopy\",\n"
        "    \"Electrocardiogram\",\n"
        "    \"Colonoscopy\",\n"
        "    \"Tonsillectomy\",\n"
        "    \"Blood glucose measurement\",\n"
        "    \"Platelet count\",\n"
        "    \"Hemoglobin A1c measurement\",\n"
        "    \"Cerebrovascular accident\",\n"
        "    \"Deep vein thrombosis\",\n"
        "    \"Congestive heart failure\",\n"
        "    \"Chronic obstructive lung disease\",\n"
        "    \"Peptic ulcer\",\n"
        "]\n"
        "\n"
        "# Remove any backup concepts that are already in the primary list\n"
        "BACKUP_CONCEPTS = [c for c in BACKUP_CONCEPTS if c not in set(CONCEPT_TERMS)]\n"
        "random.shuffle(BACKUP_CONCEPTS)\n"
        "print(f\"Primary concepts: {len(CONCEPT_TERMS)}, Backup pool: {len(BACKUP_CONCEPTS)}\")\n"
    ))

    # ---- BioPortal API Setup ----
    cells.append(md(
        "## BioPortal API Setup\n"
        "\n"
        "Configure API access and define helper functions for SNOMED CT queries."
    ))
    cells.append(code(
        "LOG_PATH = LOG_DIR / \"logs.txt\"\n"
        "\n"
        "# ============================================================\n"
        "# BioPortal API configuration\n"
        "# ============================================================\n"
        "BIOPORTAL_BASE = \"https://data.bioontology.org\"\n"
        "ONTOLOGY = \"SNOMEDCT\"\n"
        "\n"
        "# Option 1: Set via environment variable (recommended)\n"
        "# Option 2: Paste your key directly below if env var is not picked up\n"
        "BIOPORTAL_API_KEY = os.environ.get(\"BIOPORTAL_API_KEY\", \"\")\n"
        "\n"
        "# Uncomment and paste your key here if the environment variable is not detected:\n"
        "# BIOPORTAL_API_KEY = \"your-key-here\"\n"
        "\n"
        "if not BIOPORTAL_API_KEY:\n"
        "    raise EnvironmentError(\n"
        "        \"BIOPORTAL_API_KEY is not set. Either:\\n\"\n"
        "        \"  1. export BIOPORTAL_API_KEY='your-key' in your shell and restart the kernel, or\\n\"\n"
        "        \"  2. Paste your key directly in the cell above (uncomment the line).\"\n"
        "    )\n"
        "\n"
        "API_DELAY = 0.5   # seconds between API calls\n"
        "API_TIMEOUT = 30   # seconds per request\n"
        "MAX_RETRIES = 2    # retry on transient failures\n"
        "\n"
        "_api_cache = {}\n"
        "_api_call_count = 0\n"
        "\n"
        "\n"
        "def _class_uri(concept_id: str) -> str:\n"
        "    \"\"\"Build the BioPortal class URI for a SNOMED CT concept.\"\"\"\n"
        "    return f\"http://purl.bioontology.org/ontology/SNOMEDCT/{concept_id}\"\n"
        "\n"
        "\n"
        "def _encode_uri(uri: str) -> str:\n"
        "    \"\"\"URL-encode a class URI for use in BioPortal path segments.\"\"\"\n"
        "    return quote(uri, safe=\"\")\n"
        "\n"
        "\n"
        "def _extract_concept_id(class_id: str) -> str:\n"
        "    \"\"\"Extract the SNOMED concept ID from a BioPortal class @id URI.\"\"\"\n"
        "    if \"/\" in class_id:\n"
        "        return class_id.rsplit(\"/\", 1)[-1]\n"
        "    return class_id\n"
        "\n"
        "\n"
        "def _api_get(url, params=None, timeout=None):\n"
        "    \"\"\"GET with caching, rate-limiting, and retry logic.\"\"\"\n"
        "    global _api_call_count\n"
        "    cache_key = url + str(sorted((params or {}).items()))\n"
        "    if cache_key in _api_cache:\n"
        "        return _api_cache[cache_key]\n"
        "\n"
        "    timeout = timeout or API_TIMEOUT\n"
        "\n"
        "    if params is None:\n"
        "        params = {}\n"
        "    params.setdefault(\"apikey\", BIOPORTAL_API_KEY)\n"
        "    params.setdefault(\"display_links\", \"false\")\n"
        "    params.setdefault(\"display_context\", \"false\")\n"
        "\n"
        "    for attempt in range(MAX_RETRIES + 1):\n"
        "        try:\n"
        "            time.sleep(API_DELAY)\n"
        "            resp = requests.get(url, params=params, timeout=timeout)\n"
        "            resp.raise_for_status()\n"
        "            data = resp.json()\n"
        "            _api_cache[cache_key] = data\n"
        "            _api_call_count += 1\n"
        "            if _api_call_count % 20 == 0:\n"
        "                print(f\"  [{_api_call_count} API calls so far]\")\n"
        "            return data\n"
        "        except requests.exceptions.Timeout as e:\n"
        "            if attempt < MAX_RETRIES:\n"
        "                wait = (attempt + 1) * 3\n"
        "                print(f\"    Timeout, retry {attempt+1}/{MAX_RETRIES} in {wait}s...\")\n"
        "                time.sleep(wait)\n"
        "            else:\n"
        "                raise\n"
        "        except requests.exceptions.RequestException as e:\n"
        "            if attempt < MAX_RETRIES:\n"
        "                wait = (attempt + 1) * 3\n"
        "                print(f\"    Error: {e}, retry {attempt+1}/{MAX_RETRIES} in {wait}s...\")\n"
        "                time.sleep(wait)\n"
        "            else:\n"
        "                raise\n"
        "\n"
        "\n"
        "def _csv_safe(x):\n"
        "    if x is None:\n"
        "        return \"\"\n"
        "    return str(x).replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n"
        "\n"
        "\n"
        "def list_to_pipe(items):\n"
        "    \"\"\"Convert a list of strings to pipe-separated string. No truncation.\"\"\"\n"
        "    items = [str(i).replace(\"|\", \" \").strip() for i in (items or []) if str(i).strip()]\n"
        "    return \"|\".join(items)\n"
        "\n"
        "\n"
        "def semantic_tag_from_fsn(fsn: str) -> str:\n"
        "    m = re.search(r\"\\(([^()]*)\\)\\s*$\", fsn or \"\")\n"
        "    return m.group(1).strip() if m else \"UNKNOWN\"\n"
        "\n"
        "\n"
        "# ============================================================\n"
        "# SNOMED CT query functions\n"
        "# ============================================================\n"
        "\n"
        "def search_concept(term: str, exact=True):\n"
        "    \"\"\"Search for a concept by term.\n"
        "    Returns (concept_id, prefLabel) or None.\n"
        "    If exact=False, uses partial matching.\"\"\"\n"
        "    url = f\"{BIOPORTAL_BASE}/search\"\n"
        "    params = {\n"
        "        \"q\": term,\n"
        "        \"ontologies\": ONTOLOGY,\n"
        "        \"pagesize\": \"10\",\n"
        "    }\n"
        "    if exact:\n"
        "        params[\"require_exact_match\"] = \"true\"\n"
        "    data = _api_get(url, params=params)\n"
        "    for item in data.get(\"collection\", []):\n"
        "        if item.get(\"obsolete\"):\n"
        "            continue\n"
        "        class_id = item.get(\"@id\", \"\")\n"
        "        pref_label = item.get(\"prefLabel\", \"\")\n"
        "        cid = _extract_concept_id(class_id)\n"
        "        if cid:\n"
        "            return cid, pref_label\n"
        "    return None\n"
        "\n"
        "\n"
        "def get_class_info(cid: str):\n"
        "    \"\"\"Get class info including FSN and definition status.\n"
        "    Returns (fsn, definition_status).\n"
        "    The FSN (Fully Specified Name) includes the semantic tag, e.g.\n"
        "    'Selectron therapy (procedure)'. BioPortal's prefLabel returns\n"
        "    the Preferred Term (no semantic tag), so we look through synonyms\n"
        "    for the FSN which ends with '(tag)'.\"\"\"\n"
        "    try:\n"
        "        encoded = _encode_uri(_class_uri(cid))\n"
        "        url = f\"{BIOPORTAL_BASE}/ontologies/{ONTOLOGY}/classes/{encoded}\"\n"
        "        data = _api_get(url, params={\"include\": \"prefLabel,synonym,properties\"})\n"
        "        pref_label = data.get(\"prefLabel\", \"\")\n"
        "        synonyms = data.get(\"synonym\", []) or []\n"
        "\n"
        "        # --- Find the FSN (the synonym ending with a semantic tag in parens) ---\n"
        "        fsn = pref_label  # fallback\n"
        "        # Check if prefLabel itself already has the semantic tag\n"
        "        if re.search(r\"\\([^()]+\\)\\s*$\", pref_label):\n"
        "            fsn = pref_label\n"
        "        else:\n"
        "            # Search through synonyms for one that ends with (semantic_tag)\n"
        "            for syn in synonyms:\n"
        "                if isinstance(syn, str) and re.search(r\"\\([^()]+\\)\\s*$\", syn):\n"
        "                    fsn = syn\n"
        "                    break\n"
        "\n"
        "        # --- Extract definition status ---\n"
        "        props = data.get(\"properties\", {})\n"
        "        def_status_id = \"\"\n"
        "        # Search through all property keys for definition status\n"
        "        # BioPortal may use different key formats:\n"
        "        #   - 'definitionStatusId'\n"
        "        #   - 'DEFINITION_STATUS_ID'\n"
        "        #   - URI like 'http://snomed.info/.../definitionStatusId'\n"
        "        for key, val in props.items():\n"
        "            key_lower = key.lower()\n"
        "            if \"definitionstatus\" in key_lower or \"definition_status\" in key_lower:\n"
        "                def_status_id = val[0] if isinstance(val, list) and val else str(val)\n"
        "                break\n"
        "        # Debug: log property keys for the first few concepts\n"
        "        if _api_call_count <= 5:\n"
        "            prop_keys = [k.rsplit('/', 1)[-1] if '/' in k else k for k in props.keys()]\n"
        "            print(f\"    [debug] properties keys for {cid}: {prop_keys}\")\n"
        "            if def_status_id:\n"
        "                print(f\"    [debug] definition_status raw value: {def_status_id}\")\n"
        "        if \"900000000000073002\" in def_status_id:\n"
        "            def_status = \"Fully defined\"\n"
        "        elif \"900000000000074008\" in def_status_id:\n"
        "            def_status = \"Primitive\"\n"
        "        else:\n"
        "            # Try to interpret the value itself\n"
        "            val_lower = def_status_id.lower()\n"
        "            if \"defined\" in val_lower:\n"
        "                def_status = \"Fully defined\"\n"
        "            elif \"primitive\" in val_lower:\n"
        "                def_status = \"Primitive\"\n"
        "            else:\n"
        "                def_status = \"UNKNOWN\"\n"
        "        return fsn, def_status\n"
        "    except Exception as e:\n"
        "        print(f\"    WARNING: get_class_info({cid}) failed: {e}\")\n"
        "        return \"\", \"UNKNOWN\"\n"
        "\n"
        "\n"
        "def get_parents(cid: str):\n"
        "    \"\"\"Get parents. Returns list of (concept_id, prefLabel) tuples.\"\"\"\n"
        "    try:\n"
        "        encoded = _encode_uri(_class_uri(cid))\n"
        "        url = f\"{BIOPORTAL_BASE}/ontologies/{ONTOLOGY}/classes/{encoded}/parents\"\n"
        "        data = _api_get(url)\n"
        "        results = []\n"
        "        if isinstance(data, list):\n"
        "            items = data\n"
        "        else:\n"
        "            items = data.get(\"collection\", data.get(\"results\", []))\n"
        "        for item in items:\n"
        "            pid = _extract_concept_id(item.get(\"@id\", \"\"))\n"
        "            pref = item.get(\"prefLabel\", \"\")\n"
        "            if pid:\n"
        "                results.append((pid, pref))\n"
        "        return results\n"
        "    except Exception as e:\n"
        "        print(f\"    WARNING: get_parents({cid}) failed: {e}\")\n"
        "        return []\n"
        "\n"
        "\n"
        "def get_children(cid: str):\n"
        "    \"\"\"Get children. Returns list of (concept_id, prefLabel) tuples.\"\"\"\n"
        "    try:\n"
        "        encoded = _encode_uri(_class_uri(cid))\n"
        "        url = f\"{BIOPORTAL_BASE}/ontologies/{ONTOLOGY}/classes/{encoded}/children\"\n"
        "        data = _api_get(url, params={\"pagesize\": \"100\"}, timeout=20)\n"
        "        results = []\n"
        "        items = data.get(\"collection\", []) if isinstance(data, dict) else data\n"
        "        for item in items:\n"
        "            child_id = _extract_concept_id(item.get(\"@id\", \"\"))\n"
        "            pref = item.get(\"prefLabel\", \"\")\n"
        "            if child_id:\n"
        "                results.append((child_id, pref))\n"
        "        return results\n"
        "    except requests.exceptions.Timeout:\n"
        "        print(f\"    (children timeout for {cid} - likely a leaf concept)\")\n"
        "        return []\n"
        "    except Exception as e:\n"
        "        print(f\"    WARNING: get_children({cid}) failed: {e}\")\n"
        "        return []\n"
        "\n"
        "\n"
        "def get_siblings(cid: str):\n"
        "    \"\"\"Get siblings (share a parent). Returns list of prefLabel strings.\"\"\"\n"
        "    sibs = set()\n"
        "    for pid, _ in get_parents(cid):\n"
        "        for child_id, child_pref in get_children(pid):\n"
        "            if child_id != cid:\n"
        "                sibs.add(child_pref)\n"
        "    return list(sibs)\n"
        "\n"
        "\n"
        "def get_grandparents(cid: str):\n"
        "    \"\"\"Get grandparents (parents of parents, depth -2). Returns list of prefLabel strings.\"\"\"\n"
        "    gps = set()\n"
        "    for parent_id, _ in get_parents(cid):\n"
        "        for gp_id, gp_pref in get_parents(parent_id):\n"
        "            gps.add(gp_pref)\n"
        "    return list(gps)\n"
        "\n"
        "\n"
        "print(\"BioPortal API configured.\")\n"
        "print(f\"API base: {BIOPORTAL_BASE}\")\n"
        "print(f\"Ontology: {ONTOLOGY}\")\n"
    ))

    # ---- Concept Validation & GT Extraction ----
    cells.append(md(
        "## Concept Validation & Ground Truth Extraction\n"
        "\n"
        "For each concept:\n"
        "1. Try exact match in BioPortal\n"
        "2. If not found, try partial/fuzzy match\n"
        "3. If still not found, replace with a concept from the backup pool\n"
        "4. Extract ground truth relationships\n"
        "\n"
        "**Resume:** If you interrupt and re-run this cell, it loads existing\n"
        "`ground_truth.csv` and `validated_concepts.csv`, skips concepts already done,\n"
        "and continues from the next concept. Progress is saved every 10 concepts."
    ))
    cells.append(code(
        "# ============================================================\n"
        "# Validate concepts and extract ground truth\n"
        "# ============================================================\n"
        "\n"
        "backup_idx = 0\n"
        "gt_rows = []\n"
        "validation_rows = []\n"
        "not_found_terms = []\n"
        "replaced_terms = []\n"
        "\n"
        "# ---- Resume: load existing progress if present ----\n"
        "already_processed = set()\n"
        "if VALIDATED_CONCEPTS_OUT.exists():\n"
        "    _val = pd.read_csv(VALIDATED_CONCEPTS_OUT)\n"
        "    already_processed = set(_val[\"original_term\"].dropna().astype(str).str.strip())\n"
        "    validation_rows = _val.to_dict(\"records\")\n"
        "    validation_rows = [{\"concept_term\": r.get(\"concept_term\", \"\"), \"original_term\": r.get(\"original_term\", \"\"), \"snomed_id\": str(r.get(\"snomed_id\", \"\")), \"status\": r.get(\"status\", \"\")} for r in validation_rows]\n"
        "    backup_idx = sum(1 for r in validation_rows if r.get(\"status\") == \"replaced\")\n"
        "if GT_OUT.exists():\n"
        "    _gt = pd.read_csv(GT_OUT)\n"
        "    gt_rows = _gt.to_dict(\"records\")\n"
        "    gt_rows = [{k: (\"\" if pd.isna(v) else str(v)) for k, v in row.items()} for row in gt_rows]\n"
        "\n"
        "concepts_to_process = [c for c in CONCEPT_TERMS if c not in already_processed]\n"
        "\n"
        "if already_processed:\n"
        "    print(f\"Resuming: {len(already_processed)} concepts already done, {len(concepts_to_process)} remaining.\")\n"
        "print(f\"Validating and extracting GT for {len(concepts_to_process)} concepts...\")\n"
        "print(f\"Backup pool: {len(BACKUP_CONCEPTS)} concepts available (consumed so far: {backup_idx})\\n\")\n"
        "\n"
        "for i, concept_term in enumerate(concepts_to_process):\n"
        "    original_term = concept_term\n"
        "    status = \"found\"\n"
        "\n"
        "    try:\n"
        "        # Step 1: Try exact match\n"
        "        result = search_concept(concept_term, exact=True)\n"
        "\n"
        "        # Step 2: Try partial match if exact fails\n"
        "        if result is None:\n"
        "            result = search_concept(concept_term, exact=False)\n"
        "            if result:\n"
        "                status = \"partial_match\"\n"
        "                print(f\"  [{i+1}/{len(concepts_to_process)}] {concept_term} -> partial match: {result[1]}\")\n"
        "\n"
        "        # Step 3: Replace with backup concept if still not found\n"
        "        if result is None:\n"
        "            not_found_terms.append(concept_term)\n"
        "            # Try backup concepts until we find one that works\n"
        "            replacement_found = False\n"
        "            while backup_idx < len(BACKUP_CONCEPTS):\n"
        "                backup_term = BACKUP_CONCEPTS[backup_idx]\n"
        "                backup_idx += 1\n"
        "                backup_result = search_concept(backup_term, exact=True)\n"
        "                if backup_result is None:\n"
        "                    backup_result = search_concept(backup_term, exact=False)\n"
        "                if backup_result:\n"
        "                    result = backup_result\n"
        "                    concept_term = backup_term\n"
        "                    status = \"replaced\"\n"
        "                    replaced_terms.append((original_term, backup_term))\n"
        "                    print(f\"  [{i+1}/{len(concepts_to_process)}] {original_term} -> NOT FOUND, replaced with: {backup_term} ({result[0]})\")\n"
        "                    replacement_found = True\n"
        "                    break\n"
        "            if not replacement_found:\n"
        "                print(f\"  [{i+1}/{len(concepts_to_process)}] {original_term} -> NOT FOUND (no backup available)\")\n"
        "                validation_rows.append({\n"
        "                    \"concept_term\": original_term,\n"
        "                    \"original_term\": original_term,\n"
        "                    \"snomed_id\": \"\",\n"
        "                    \"status\": \"not_found\",\n"
        "                })\n"
        "                with LOG_PATH.open(\"a\") as f:\n"
        "                    f.write(f\"{datetime.now().isoformat()}\\t{original_term}\\tNOT_FOUND\\n\")\n"
        "                continue\n"
        "\n"
        "        cid, pref_label = result\n"
        "\n"
        "        # Extract ground truth\n"
        "        fsn, def_status = get_class_info(cid)\n"
        "        if not fsn:\n"
        "            fsn = pref_label  # fallback to search prefLabel\n"
        "        sem_tag = semantic_tag_from_fsn(fsn)\n"
        "\n"
        "        parent_tuples = get_parents(cid)\n"
        "        child_tuples = get_children(cid)\n"
        "        parent_labels = [lbl for _, lbl in parent_tuples]\n"
        "        child_labels = [lbl for _, lbl in child_tuples]\n"
        "        sib_labels = get_siblings(cid)\n"
        "        gp_labels = get_grandparents(cid)\n"
        "\n"
        "        gt_rows.append({\n"
        "            \"timestamp\": datetime.now().isoformat(),\n"
        "            \"concept_term\": concept_term,\n"
        "            \"snomed_id\": cid,\n"
        "            \"fsn\": _csv_safe(fsn),\n"
        "            \"semantic_tag\": _csv_safe(sem_tag),\n"
        "            \"definition_status\": _csv_safe(def_status),\n"
        "            \"parents\": list_to_pipe(parent_labels),\n"
        "            \"grandparents\": list_to_pipe(gp_labels),\n"
        "            \"children\": list_to_pipe(child_labels),\n"
        "            \"siblings\": list_to_pipe(sib_labels),\n"
        "        })\n"
        "\n"
        "        validation_rows.append({\n"
        "            \"concept_term\": concept_term,\n"
        "            \"original_term\": original_term,\n"
        "            \"snomed_id\": cid,\n"
        "            \"status\": status,\n"
        "        })\n"
        "\n"
        "        if status == \"found\":\n"
        "            print(f\"  [{i+1}/{len(concepts_to_process)}] {concept_term} -> {cid} \"\n"
        "                  f\"({len(parent_labels)}P, {len(gp_labels)}GP, {len(child_labels)}C, {len(sib_labels)}S)\")\n"
        "\n"
        "        with LOG_PATH.open(\"a\") as f:\n"
        "            f.write(\n"
        "                \"\\n\" + \"=\" * 80 + \"\\n\" + datetime.now().isoformat() + \"\\n\"\n"
        "                + f\"CONCEPT: {concept_term} (original: {original_term})\\n\"\n"
        "                + f\"STATUS: {status}\\n\"\n"
        "                + f\"SNOMED_ID: {cid}\\nFSN: {fsn}\\n\"\n"
        "                + f\"TAG: {sem_tag}\\nDEF_STATUS: {def_status}\\n\"\n"
        "                + f\"PARENTS: {parent_labels}\\nGRANDPARENTS: {gp_labels}\\n\"\n"
        "                + f\"CHILDREN: {child_labels}\\nSIBLINGS ({len(sib_labels)}): {sib_labels[:10]}...\\n\"\n"
        "            )\n"
        "\n"
        "    except Exception as e:\n"
        "        with LOG_PATH.open(\"a\") as f:\n"
        "            f.write(f\"{datetime.now().isoformat()}\\t{original_term}\\tERROR\\t{e}\\n\")\n"
        "        print(f\"  [{i+1}/{len(concepts_to_process)}] {original_term} -> ERROR: {e}\")\n"
        "\n"
        "    # --- Incremental save every 10 concepts ---\n"
        "    if gt_rows and (i + 1) % 10 == 0:\n"
        "        _tmp_gt = pd.DataFrame(gt_rows)\n"
        "        _tmp_gt = _tmp_gt[[\n"
        "            \"timestamp\", \"concept_term\", \"snomed_id\",\n"
        "            \"fsn\", \"semantic_tag\", \"definition_status\",\n"
        "            \"parents\", \"grandparents\", \"children\", \"siblings\",\n"
        "        ]]\n"
        "        _tmp_gt.to_csv(GT_OUT, index=False)\n"
        "        _tmp_val = pd.DataFrame(validation_rows)\n"
        "        if not _tmp_val.empty:\n"
        "            _tmp_val[[\"concept_term\", \"original_term\", \"snomed_id\", \"status\"]].to_csv(VALIDATED_CONCEPTS_OUT, index=False)\n"
        "        print(f\"    [checkpoint] Saved {len(gt_rows)} concepts to CSV\")\n"
        "\n"
        "print(f\"\\nDone. Total API calls: {_api_call_count}\")\n"
        "print(f\"Found: {sum(1 for r in validation_rows if r['status'] == 'found')}\")\n"
        "print(f\"Partial match: {sum(1 for r in validation_rows if r['status'] == 'partial_match')}\")\n"
        "print(f\"Replaced: {sum(1 for r in validation_rows if r['status'] == 'replaced')}\")\n"
        "print(f\"Not found (no replacement): {sum(1 for r in validation_rows if r['status'] == 'not_found')}\")\n"
    ))

    # ---- Save Results ----
    cells.append(md("## Save Results"))
    cells.append(code(
        "# ============================================================\n"
        "# Save ground truth CSV\n"
        "# ============================================================\n"
        "if gt_rows:\n"
        "    gt_df = pd.DataFrame(gt_rows)\n"
        "    gt_df = gt_df[[\n"
        "        \"timestamp\", \"concept_term\", \"snomed_id\",\n"
        "        \"fsn\", \"semantic_tag\", \"definition_status\",\n"
        "        \"parents\", \"grandparents\", \"children\", \"siblings\",\n"
        "    ]]\n"
        "    gt_df.to_csv(GT_OUT, index=False)\n"
        "    print(f\"Ground truth saved: {GT_OUT} ({len(gt_df)} concepts)\")\n"
        "else:\n"
        "    print(\"WARNING: No ground truth rows to save!\")\n"
        "\n"
        "# ============================================================\n"
        "# Save validated concepts CSV\n"
        "# ============================================================\n"
        "if validation_rows:\n"
        "    val_df = pd.DataFrame(validation_rows)\n"
        "    val_df = val_df[[\"concept_term\", \"original_term\", \"snomed_id\", \"status\"]]\n"
        "    val_df.to_csv(VALIDATED_CONCEPTS_OUT, index=False)\n"
        "    print(f\"Validated concepts saved: {VALIDATED_CONCEPTS_OUT} ({len(val_df)} concepts)\")\n"
        "else:\n"
        "    print(\"WARNING: No validated concepts to save!\")\n"
        "\n"
        "# ============================================================\n"
        "# Summary (from full validation_rows so resume runs are included)\n"
        "# ============================================================\n"
        "_replaced = [(r[\"original_term\"], r[\"concept_term\"]) for r in validation_rows if r.get(\"status\") == \"replaced\"]\n"
        "_not_found = [r[\"original_term\"] for r in validation_rows if r.get(\"status\") == \"not_found\"]\n"
        "if _replaced:\n"
        "    print(\"\\nReplaced concepts:\")\n"
        "    for orig, repl in _replaced:\n"
        "        print(f\"  {orig} -> {repl}\")\n"
        "\n"
        "if _not_found:\n"
        "    print(\"\\nCould not find or replace:\")\n"
        "    for t in _not_found:\n"
        "        print(f\"  {t}\")\n"
        "\n"
        "if not _replaced and not _not_found:\n"
        "    pass  # no replaced/not_found to show\n"
        "\n"
        "print(\"\\n\" + \"=\" * 80)\n"
        "print(\"Step 1 complete. Ground truth is shared by all LLM folders.\")\n"
        "print(\"Now run step2_llm_queries.ipynb in each testing_* folder.\")\n"
        "print(\"=\" * 80)\n"
    ))

    return cells


def main():
    nb = {
        "cells": build_cells(),
        "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "3.10.0"},
        },
        "nbformat": 4,
        "nbformat_minor": 4,
    }
    # Write to ground_truth/ folder at repo root (single shared notebook)
    gt_dir = REPO / "ground_truth"
    gt_dir.mkdir(parents=True, exist_ok=True)
    path = gt_dir / "step1_ground_truth.ipynb"
    with open(path, "w") as f:
        json.dump(nb, f, indent=2)
    print(f"Wrote {path}")


if __name__ == "__main__":
    main()
